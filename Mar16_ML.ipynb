{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9e2c0c-291c-4b9c-8cf0-adae407f4e16",
   "metadata": {},
   "source": [
    "###  Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc514a5-8316-4601-a9ff-1472e14a56c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Before building a machine-learning model it is common to split the whole dataset into two sub-datasets: Training data and Test data. The training data as the name suggests is used to train the machine-learning model to find the patterns and relationships in the data. The trained model is then used on the test dataset to make the predictions.\n",
    "\n",
    "- Overfitting occurs when the model is very complex and fits the training data very closely. This will result in poor generalization of the model. This means the model performs well on training data but it will not be able to predict accurate outcomes for new, unseen data.This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples.\n",
    "\n",
    "- Underfitting occurs when a model is too simple and is unable to properly capture the patterns and relationships in the data. This means the model will perform poorly on both the training and the test data. Model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y). \n",
    "\n",
    "Overfitting is often caused by using a model with too many parameters or if the model is too powerful for the given dataset. On the other hand, underfitting is often caused by the model with too few parameters or by using a model that is not powerful enough for the given dataset.\n",
    "\n",
    "- Bias and Variance in Machine Learning\n",
    "\n",
    "Bias: Bias refers to the error due to overly simplistic assumptions in the learning algorithm. These assumptions make the model easier to comprehend and learn but might not capture the underlying complexities of the data. It is the error due to the model’s inability to represent the true relationship between input and output accurately. When a model has poor performance both on the training and testing data means high bias because of the simple model, indicating underfitting.\n",
    "\n",
    "Variance: Variance, on the other hand, is the error due to the model’s sensitivity to fluctuations in the training data. It’s the variability of the model’s predictions for different instances of training data. High variance occurs when a model learns the training data’s noise and random fluctuations rather than the underlying pattern. As a result, the model performs well on the training data but poorly on the testing data, indicating overfitting.\n",
    "\n",
    "- Reasons for Underfitting\n",
    "1. The model is too simple, So it may be not capable to represent the complexities in the data.\n",
    "2. The input features which is used to train the model is not the adequate representations of underlying factors influencing the target variable.\n",
    "3. The size of the training dataset used is not enough.\n",
    "4. Excessive regularization are used to prevent the overfitting, which constraint the model to capture the data well.\n",
    "5. Features are not scaled.\n",
    "\n",
    "- Techniques to Reduce Underfitting\n",
    "1. Increase model complexity.\n",
    "2. Increase the number of features, performing feature engineering.\n",
    "3. Remove noise from the data.\n",
    "4. Increase the number of epochs or increase the duration of training to get better results.\n",
    "\n",
    "- Reasons for Overfitting:\n",
    "1. High variance and low bias.\n",
    "2. The model is too complex.\n",
    "3. The size of the training data.\n",
    "\n",
    "- Techniques to Reduce Overfitting\n",
    "1. Increase training data.\n",
    "2. Reduce model complexity.\n",
    "3. Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
    "4. Ridge Regularization and Lasso Regularization.\n",
    "5. Use dropout for neural networks to tackle overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c86b06-c8e7-45ac-9ebd-fa5245e7fd00",
   "metadata": {},
   "source": [
    "### Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c691a46-e1f7-477e-9a00-628630515307",
   "metadata": {},
   "source": [
    "#### You can prevent overfitting by diversifying and scaling your training data set or using some other data science strategies, like those given below.\n",
    "Early stopping: Early stopping pauses the training phase before the machine learning model learns the noise in the data. However, getting the timing right is important; else the model will still not give accurate results.\n",
    "- Pruning: You might identify several features or parameters that impact the final prediction when you build a model. Feature selection—or pruning—identifies the most important features within the training set and eliminates irrelevant ones. For example, to predict if an image is an animal or human, you can look at various input parameters like face shape, ear position, body structure, etc. You may prioritize face shape and ignore the shape of the eyes.\n",
    "- Regularization: Regularization is a collection of training/optimization techniques that seek to reduce overfitting. These methods try to eliminate those factors that do not impact the prediction outcomes by grading features based on importance. For example, mathematical calculations apply a penalty value to features with minimal impact. Consider a statistical model attempting to predict the housing prices of a city in 20 years. Regularization would give a lower penalty value to features like population growth and average annual income but a higher penalty value to the average annual temperature of the city.\n",
    "- Ensembling: Ensembling combines predictions from several separate machine learning algorithms. Some models are called weak learners because their results are often inaccurate. Ensemble methods combine all the weak learners to get more accurate results. They use multiple models to analyze sample data and pick the most accurate outcomes. The two main ensemble methods are bagging and boosting. Boosting trains different machine learning models one after another to get the final result, while bagging trains them in parallel.\n",
    "- Data augmentation: Data augmentation is a machine learning technique that changes the sample data slightly every time the model processes it. You can do this by changing the input data in small ways. When done in moderation, data augmentation makes the training sets appear unique to the model and prevents the model from learning their characteristics. For example, applying transformations such as translation, flipping, and rotation to input images.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9826ea7-6912-471c-8220-8090898898be",
   "metadata": {},
   "source": [
    "###  Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a4776-5c00-40a6-a520-21719d68cd3e",
   "metadata": {},
   "source": [
    "#### When a model has not learned the patterns in the training data well and is unable to generalize well on the new data, it is known as underfitting. An underfit model has poor performance on the training data and will result in unreliable predictions. \n",
    "\n",
    "It occurs when a model is too simple, which can be a result of a model needing more training time, more input features, or less regularization. Like overfitting, when a model is underfitted, it cannot establish the dominant trend within the data, resulting in training errors and poor performance of the model.\n",
    "\n",
    "- Scenarios for underfitting:\n",
    "\n",
    "1. The model is too simple, So it may be not capable to represent the complexities in the data.\n",
    "2. The input features which is used to train the model is not the adequate representations of underlying factors influencing the target variable.\n",
    "3. The size of the training dataset used is not enough.\n",
    "4. Excessive regularization are used to prevent the overfitting, which constraint the model to capture the data well.\n",
    "5. Features are not scaled.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9d1166-20df-4850-a685-ef80e3f39c41",
   "metadata": {},
   "source": [
    "###  Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0086f7-4a1c-46d4-9bff-496c26863f94",
   "metadata": {},
   "source": [
    "#### \n",
    "Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data.\n",
    "\n",
    "Variance is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data.\n",
    "\n",
    "- Bias-Variance Tradeoff: If our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand if our model has large number of parameters then it’s going to have high variance and low bias. So we need to find the right/good balance without overfitting and underfitting the data. This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time.\n",
    "\n",
    "To build a good model, we need to find a good balance between bias and variance such that it minimizes the total error.\n",
    "\n",
    "\n",
    "Total error = Bias^2 + Variance + irreducible error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b95343-58bf-4917-9428-fcf9ed8ce62d",
   "metadata": {},
   "source": [
    "###  Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e3f51-5d26-4c9d-b7d0-e1bc1a35bc38",
   "metadata": {},
   "source": [
    "#### \n",
    "- To avoid overfitting we can divide our dataset into random train and test subsets. If your model performs much better on the training set than on the test set, then you’re likely overfitting. For example, it would be a big warning if your model saw 80% accuracy on the training set but only 50% accuracy on the test set. Then, automatically it would be a red flag for the model.\n",
    "- Another way to detect overfitting is to start with a simplistic baseline model that will serve as a benchmark. This approach is known as Occam’s Razor Test. Occam razor is attributed to English Franciscan friar William of Ockham (c. 1287–1347). It basically selects the simplistic model in case of comparable performance in case of two models.\n",
    "According to approach, a simple machine learning model can predict better data insights in comparison with excessive complex machine learning models that usually suffer with statistics noise known as bias variance tradeoff. But sometimes it’s very hard to decide which part of data is noise.\n",
    "- K-fold cross-validation\n",
    "Cross-validation is one of the testing methods used in practice. In this method, data scientists divide the training set into K equally sized subsets or sample sets called folds. The training process consists of a series of iterations. During each iteration, the steps are:\n",
    "1.    Keep one subset as the validation data and train the machine learning model on the remaining K-1 subsets.\n",
    "2.    Observe how the model performs on the validation sample.\n",
    "3.    Score model performance based on output data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b389634d-fcc3-4396-9373-0dd5fe9e0a36",
   "metadata": {},
   "source": [
    "### Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95811ecc-ea8f-4ebc-bb5d-492286c576b0",
   "metadata": {},
   "source": [
    "##### Bias is simply defined as the inability of the model because of that there is some difference or error occurring between the model’s predicted value and the actual value. These differences between actual or expected values and the predicted values are known as error or bias error or error due to bias. Bias is a systematic error that occurs due to wrong assumptions in the machine learning process. The high-bias model will not be able to capture the dataset trend. It is considered as the underfitting model which has a high error rate. It is due to a very simplified algorithm.\n",
    "\n",
    "- Low Bias: Low bias value means fewer assumptions are taken to build the target function. In this case, the model will closely match the training dataset.\n",
    "- High Bias: High bias value means more assumptions are taken to build the target function. In this case, the model will not match the training dataset closely. \n",
    "\n",
    "##### Variance is the measure of spread in data from its mean position. In machine learning variance is the amount by which the performance of a predictive model changes when it is trained on different subsets of the training data. More specifically, variance is the variability of the model that how much it is sensitive to another subset of the training dataset. i.e. how much it can adjust on the new subset of the training dataset.\n",
    "\n",
    "- Low variance: Low variance means that the model is less sensitive to changes in the training data and can produce consistent estimates of the target function with different subsets of data from the same distribution. This is the case of underfitting when the model fails to generalize on both training and test data.\n",
    "- High variance: High variance means that the model is very sensitive to changes in the training data and can result in significant changes in the estimate of the target function when trained on different subsets of data from the same distribution. This is the case of overfitting when the model performs well on the training data but poorly on new, unseen test data. It fits the training data too closely that it fails on the new training dataset.\n",
    "\n",
    "A model with high variance may represent the data set accurately but could lead to overfitting to noisy or otherwise unrepresentative training data. In comparison, a model with high bias may underfit the training data due to a simpler model that overlooks regularities in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58375be-68b7-4f11-bed1-684ad3045036",
   "metadata": {},
   "source": [
    "###  Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c2668d-bacb-490d-8301-8d7f2e09664c",
   "metadata": {},
   "source": [
    "##### Regularization is a technique used to reduce errors by fitting the function appropriately on the given training set and avoiding overfitting. The commonly used regularization techniques are : \n",
    "\n",
    "- Lasso Regularization – L1 Regularization\n",
    "A regression model which uses the L1 Regularization technique is called LASSO(Least Absolute Shrinkage and Selection Operator) regression. Lasso Regression adds the “absolute value of magnitude” of the coefficient as a penalty term to the loss function(L). Lasso regression also helps us achieve feature selection by penalizing the weights to approximately equal to zero if that feature does not serve any purpose in the model.\n",
    "- Ridge Regularization – L2 Regularization\n",
    "A regression model that uses the L2 regularization technique is called Ridge regression. Ridge regression adds the “squared magnitude” of the coefficient as a penalty term to the loss function(L).\n",
    "- Elastic Net Regularization – L1 and L2 Regularization\n",
    "This model is a combination of L1 as well as L2 regularization. That implies that we add the absolute norm of the weights as well as the squared measure of the weights. With the help of an extra hyperparameter that controls the ratio of the L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0474613d-32dc-44e5-a38a-565dd0c539c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
